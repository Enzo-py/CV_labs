{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lNWDh5S8zjw"
      },
      "source": [
        "In a new python environment with python>=3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sDlD3KX8zjy"
      },
      "outputs": [],
      "source": [
        "!pip install \"torch_uncertainty[image] @ git+https://github.com/ENSTA-U2IS-AI/torch-uncertainty@dev\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcLeda8S8zjz"
      },
      "outputs": [],
      "source": [
        "# here are the training parameters\n",
        "batch_size = 10\n",
        "learning_rate =1e-3\n",
        "weight_decay=2e-4\n",
        "lr_decay_epochs=20\n",
        "lr_decay=0.1\n",
        "nb_epochs=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TqJpiUX8zjz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "from torch_uncertainty.datasets import MUAD\n",
        "\n",
        "train_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize(size=(256, 512), antialias=True),\n",
        "        v2.RandomHorizontalFlip(),\n",
        "        v2.ToDtype(\n",
        "            dtype={\n",
        "                tv_tensors.Image: torch.float32,\n",
        "                tv_tensors.Mask: torch.int64,\n",
        "                \"others\": None,\n",
        "            },\n",
        "            scale=True,\n",
        "        ),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = v2.Compose(\n",
        "    [\n",
        "        v2.Resize(size=(256, 512), antialias=True),\n",
        "        v2.ToDtype(\n",
        "            dtype={\n",
        "                tv_tensors.Image: torch.float32,\n",
        "                tv_tensors.Mask: torch.int64,\n",
        "                \"others\": None,\n",
        "            },\n",
        "            scale=True,\n",
        "        ),\n",
        "        v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"train\" , transforms=train_transform, download=True)\n",
        "val_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"val\" , transforms=val_transform, download=True)\n",
        "test_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"test\" , transforms=val_transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9qKz4D38zj0"
      },
      "source": [
        "Let us see the first sample of the validation set. The first image is the input and the second image is the target (ground truth)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rgOIk4B8zj0"
      },
      "outputs": [],
      "source": [
        "sample = train_set[0]\n",
        "img, tgt = sample\n",
        "img.size(), tgt.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8mlTc2u8zj1"
      },
      "source": [
        "Visualize a validation input sample (and RGB image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5EF4vBD8zj1"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "F.to_pil_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKaF3mu8zj1"
      },
      "source": [
        "Visualize the same image above but segmented (our goal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgTqq-hN8zj2"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import draw_segmentation_masks\n",
        "\n",
        "tmp_tgt = tgt.masked_fill(tgt == 255, 21)\n",
        "tgt_masks = tmp_tgt == torch.arange(22, device=tgt.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, tgt_masks, alpha=1, colors=val_set.color_palette)\n",
        "F.to_pil_image(img_segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K21284qq8zj2"
      },
      "source": [
        "Below is the complete list of classes in MUAD, presented as:\n",
        "\n",
        "1.   Class Name\n",
        "2.   Train ID\n",
        "3.   Segmentation Color in RGB format [R,G, B]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CBuq3FL8zj3"
      },
      "outputs": [],
      "source": [
        "for muad_class in train_set.classes:\n",
        "    class_name = muad_class.name\n",
        "    train_id = muad_class.id\n",
        "    color = muad_class.color\n",
        "    print(f\"Class: {class_name}, Train ID: {train_id}, Color: {color}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM6QQ8GP8zj3"
      },
      "source": [
        "Here is a more comprhensive review of the diffrent classes : (while training Non-labeled data will use train ID 21 and not 255)\n",
        "\n",
        "\n",
        "| **class names**                       | **ID** |\n",
        "|----------------------------------------|---------|\n",
        "| road                                   | 0       |\n",
        "| sidewalk                               | 1       |\n",
        "| building                               | 2       |\n",
        "| wall                                   | 3       |\n",
        "| fence                                  | 4       |\n",
        "| pole                                   | 5       |\n",
        "| traffic light                          | 6       |\n",
        "| traffic sign                           | 7       |\n",
        "| vegetation                             | 8       |\n",
        "| terrain                                | 9       |\n",
        "| sky                                    | 10      |\n",
        "| person                                 | 11      |\n",
        "| rider                                  | 12      |\n",
        "| car                                    | 13      |\n",
        "| truck                                  | 14      |\n",
        "| bus                                    | 15      |\n",
        "| train                                  | 16      |\n",
        "| motorcycle                             | 17      |\n",
        "| bicycle                                | 18      |\n",
        "| bear deer cow                          | 19      |\n",
        "| garbage_bag stand_food trash_can       | 20      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IBM8fzf8zj3"
      },
      "source": [
        "We will feed our DNN the first raw image of the road view and as target it will be the dark image below and not the colored one (second image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU4A6w428zj3"
      },
      "outputs": [],
      "source": [
        "im = F.to_pil_image(F.to_dtype(tgt, torch.uint8))\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLRSaJgV8zj3"
      },
      "outputs": [],
      "source": [
        "im.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdF1nx6T8zj4"
      },
      "source": [
        "**Why is the target image dark and what's the bright part ?** **(hint : print the numpy array)**\n",
        "\n",
        "**answer:** The target is dark because it is not an RGB image but a class label map, where pixel values are small integers (0â€“21), which appear nearly black in grayscale. The bright regions correspond to pixels with value 255, which indicate ignored / unlabeled areas in the segmentation mask."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = tgt.squeeze(0).cpu().numpy()\n",
        "print(arr.dtype, arr.min(), arr.max())\n",
        "print(\"unique values:\", np.unique(arr)[:50], \"...\")\n",
        "print(\"count of 255:\", (arr == 255).sum())\n",
        "print(\"count of 0:\", (arr == 0).sum())"
      ],
      "metadata": {
        "id": "g9F-30gUm65x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbrvwC98zj4"
      },
      "source": [
        "\\**Q3/ please study the dataset a bit. What it is about?**\n",
        "\n",
        "**answer:** The MUAD dataset is a semantic segmentation dataset for autonomous driving scenes, where each image corresponds to an urban road view and each pixel is assigned a semantic class (road, vehicle, pedestrian, sky, vegetation, etc.). It is designed to train and evaluate models that understand scene layout at pixel level, a core requirement for perception in self-driving and driver-assistance systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivjRh97e8zj4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4)\n",
        "\n",
        "val_loader = DataLoader(\n",
        "        val_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn9_b_KT8zj4"
      },
      "outputs": [],
      "source": [
        "def enet_weighing(dataloader, num_classes, c=1.02):\n",
        "    \"\"\"Computes class weights as described in the ENet paper.\n",
        "\n",
        "        w_class = 1 / (ln(c + p_class)),\n",
        "\n",
        "    where c is usually 1.02 and p_class is the propensity score of that\n",
        "    class:\n",
        "\n",
        "        propensity_score = freq_class / total_pixels.\n",
        "\n",
        "    References:\n",
        "        https://arxiv.org/abs/1606.02147\n",
        "\n",
        "    Args:\n",
        "        dataloader (``data.Dataloader``): A data loader to iterate over the\n",
        "            dataset.\n",
        "        num_classes (``int``): The number of classes.\n",
        "        c (``int``, optional): AN additional hyper-parameter which restricts\n",
        "            the interval of values for the weights. Default: 1.02.\n",
        "\n",
        "    \"\"\"\n",
        "    class_count = 0\n",
        "    total = 0\n",
        "    for _, label in dataloader:\n",
        "      label = label.cpu().numpy()\n",
        "      # Flatten label\n",
        "      flat_label = label.flatten()\n",
        "      flat_label = flat_label[flat_label != 255]\n",
        "\n",
        "      # Sum up the number of pixels of each class and the total pixel\n",
        "      # counts for each label\n",
        "      class_count += np.bincount(flat_label, minlength=num_classes)\n",
        "      total += flat_label.size\n",
        "\n",
        "    # Compute propensity score and then the weights for each class\n",
        "    propensity_score = class_count / total\n",
        "    return 1 / (np.log(c + propensity_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfrMULrG8zj4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nComputing class weights...\")\n",
        "print(\"(this can take a while depending on the dataset size)\")\n",
        "class_weights = enet_weighing(train_loader, 19)\n",
        "class_weights = torch.from_numpy(class_weights).float().cuda()\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcyhmI8m8zj5"
      },
      "source": [
        "**Q4/ why do we need to evaluate the class_weights?**\n",
        "\n",
        "**answer:** Because the class are unbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkKjz_k8zj5"
      },
      "source": [
        "## C. building the DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g46XoY1P8zj5"
      },
      "source": [
        "**Q5/ Do we really use Unet? What did I change :)? (that is hard)**\n",
        "\n",
        "**answer:** You add 2d dropout and bilinear instead of transposed conv.\n",
        "\n",
        "\n",
        "**Q6/Do we need a backbone with Unet?**\n",
        "\n",
        "**answer:** No. It could just help to learn faster if the domain gap is small.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn_vEBFM8zj5"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(conv => BN => ReLU) * 2.\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class InConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mpconv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super().__init__()\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        if self.bilinear:\n",
        "            x1 = F.resize(x1, size=[2*x1.size()[2],2*x1.size()[3]],\n",
        "                          interpolation=v2.InterpolationMode.BILINEAR)\n",
        "        else:\n",
        "            x1 = self.up(x1)\n",
        "\n",
        "        # input is CHW\n",
        "        diff_y = x2.size()[2] - x1.size()[2]\n",
        "        diff_x = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
        "                        diff_y // 2, diff_y - diff_y // 2])\n",
        "\n",
        "        # for padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "#please note that we have added dropout layer to be abble to use MC dropout\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, classes):\n",
        "        super().__init__()\n",
        "        self.inc = InConv(3, 32)\n",
        "        self.down1 = Down(32, 64)\n",
        "        self.down2 = Down(64, 128)\n",
        "        self.down3 = Down(128, 256)\n",
        "        self.down4 = Down(256, 256)\n",
        "        self.up1 = Up(512, 128)\n",
        "        self.up2 = Up(256, 64)\n",
        "        self.up3 = Up(128, 32)\n",
        "        self.up4 = Up(64, 32)\n",
        "        self.dropout = nn.Dropout2d(0.1)\n",
        "        self.outc = OutConv(32, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.dropout(x)\n",
        "        return self.outc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9xW-iRN8zj6"
      },
      "source": [
        "## D. Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n2-8wIB8zj6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Colors from Colorbrewer Paired_12\n",
        "colors = [[31, 120, 180], [51, 160, 44]]\n",
        "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
        "\n",
        "def plot_losses(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Evolution of the training and validation loss\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_accu(train_history, val_history):\n",
        "    x = np.arange(1, len(train_history) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, train_history, color=colors[0], label=\"Training miou\", linewidth=2)\n",
        "    plt.plot(x, val_history, color=colors[1], label=\"Validation miou\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Mean IoU\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Evolution of Miou\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0gLFWkM8zj7"
      },
      "source": [
        "**Q7/  what is the IoU?**\n",
        "\n",
        "**answer:** Intersection over Union measures the overlap between the predicted segmentation and the ground truth by dividing their intersection by their union.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWD37D48zj7"
      },
      "source": [
        "### Training function\n",
        "\n",
        "**Q8/Please complete the training and the test function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ETHIIe68zj7"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.utilities.compute import _safe_divide\n",
        "\n",
        "\n",
        "def train( model, data_loader, optim, criterion, metric,iteration_loss=False):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for step, batch_data in enumerate(data_loader):\n",
        "        # Get the inputs and labels\n",
        "        img = batch_data[0].cuda()\n",
        "        labels = batch_data[1].cuda()\n",
        "\n",
        "        # Forward propagation\n",
        "        outputs = model(img)\n",
        "\n",
        "        flatten_logits = outputs.permute(0, 2, 3, 1).reshape(-1, outputs.shape[1])\n",
        "        flatten_labels = labels.view(-1)\n",
        "        valid_mask = flatten_labels != 255\n",
        "\n",
        "        # Loss computation\n",
        "        loss = criterion(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        # Backpropagation\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of the evaluation metric\n",
        "        metric.update(flatten_logits[valid_mask].detach(), flatten_labels[valid_mask].detach())\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "\n",
        "    # Compute IoU per class\n",
        "    tp, fp, _, fn = metric._final_state()\n",
        "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
        "\n",
        "    return epoch_loss / len(data_loader), iou_per_class, metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiMNyCRY8zj8"
      },
      "source": [
        "### Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_UTKVIP8zj8"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader, criterion, metric, iteration_loss=False):\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for step, batch_data in enumerate(data_loader):\n",
        "        # Get the inputs and labels\n",
        "        img = batch_data[0].cuda()\n",
        "        labels = batch_data[1].cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward propagation\n",
        "            outputs = model(img)\n",
        "\n",
        "            flatten_logits = outputs.permute(0, 2, 3, 1).reshape(-1, outputs.shape[1])\n",
        "            flatten_labels = labels.view(-1)\n",
        "            valid_mask = flatten_labels != 255\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        # Keep track of loss for current epoch\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Keep track of evaluation the metric\n",
        "        metric.update(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
        "\n",
        "        if iteration_loss:\n",
        "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
        "\n",
        "    # Compute IoU per class\n",
        "    tp, fp, _, fn = metric._final_state()\n",
        "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
        "\n",
        "    return epoch_loss / len(data_loader), iou_per_class, metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiwqqSLU8zj8"
      },
      "source": [
        "## E. Training Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzAQQAS98zj9"
      },
      "source": [
        "**Q9/ please train your DNN and comment?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3QBxf8N8zj9"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torch_uncertainty.metrics import MeanIntersectionOverUnion\n",
        "\n",
        "print(\"\\nTraining...\\n\")\n",
        "num_classes = 19\n",
        "# Intialize UNet\n",
        "\n",
        "# We are going to use the CrossEntropyLoss loss function as it's most\n",
        "# frequentely used in classification problems with multiple classes which\n",
        "# fits the problem. This criterion  combines LogSoftMax and NLLLoss.\n",
        "\n",
        "model = UNet(classes=num_classes).cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=255)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay_epochs, gamma=lr_decay)\n",
        "metric = MeanIntersectionOverUnion(num_classes=num_classes, ignore_index=255).cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWqD3h2Q8zj9"
      },
      "outputs": [],
      "source": [
        "# Start Training\n",
        "train_losses, val_losses = [], []\n",
        "train_mious, val_mious = [], []\n",
        "train_ious, val_ious = [], []\n",
        "\n",
        "best_val_miou = -1.0\n",
        "best_state = None\n",
        "\n",
        "# used 15 epoch rather than initial nb_epochs cuz too long on my google colab...\n",
        "nb_epochs = 20\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "    tr_loss, tr_iou_per_class, tr_miou = train(\n",
        "        model, train_loader, optimizer, criterion, metric, iteration_loss=False\n",
        "    )\n",
        "    va_loss, va_iou_per_class, va_miou = test(\n",
        "        model, val_loader, criterion, metric, iteration_loss=False\n",
        "    )\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_losses.append(tr_loss)\n",
        "    val_losses.append(va_loss)\n",
        "    train_mious.append(tr_miou.item() if hasattr(tr_miou, \"item\") else float(tr_miou))\n",
        "    val_mious.append(va_miou.item() if hasattr(va_miou, \"item\") else float(va_miou))\n",
        "    train_ious.append(tr_iou_per_class.detach().cpu())\n",
        "    val_ious.append(va_iou_per_class.detach().cpu())\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch:03d}/{nb_epochs:03d}] \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.2e} | \"\n",
        "        f\"train: loss={tr_loss:.4f}, mIoU={train_mious[-1]:.4f} | \"\n",
        "        f\"val: loss={va_loss:.4f}, mIoU={val_mious[-1]:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_mious[-1] > best_val_miou:\n",
        "        best_val_miou = val_mious[-1]\n",
        "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "# restore best model (optional but usually expected)\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "print(f\"\\nBest val mIoU: {best_val_miou:.4f}\")\n",
        "\n",
        "# save model\n",
        "torch.save(model.state_dict(), \"model2.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pOhpPHK8zj-"
      },
      "source": [
        "Load a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8E3A5gi8zj-"
      },
      "outputs": [],
      "source": [
        "#Loading a model\n",
        "model = UNet(19)\n",
        "model.load_state_dict(torch.load(\"model1.pth\"))\n",
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsDiU4G_8zj-"
      },
      "source": [
        "# III. Evalution of the Trained DNN on the test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djpEkMSF8zkB"
      },
      "source": [
        "## A. classical evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv5mjGFp8zkC"
      },
      "source": [
        "**Q10/ please plot the loss and miou and comment about it ?**\n",
        "Both the training and test loss curves show a similar downward trend, stabilizing around 0.3. However, there's a notable difference in the mean Intersection over Union (mIoU) values: the training mIoU reaches approximately 0.75, whereas the test mIoU plateau at around 0.65. This discrepancy indicates a definite overfitting to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N11rmcci8zkC"
      },
      "outputs": [],
      "source": [
        "train_loss_history = np.array(train_losses)\n",
        "val_loss_history = np.array(val_losses)\n",
        "train_miou_history = np.array(train_mious)\n",
        "val_miou_history = np.array(val_mious)\n",
        "plot_losses(train_loss_history, val_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV-GoYE78zkC"
      },
      "outputs": [],
      "source": [
        "plot_accu(train_miou_history, val_miou_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfAjk5J98zkC"
      },
      "source": [
        "**Q11/ what should we have done to avoid overfitting?**\n",
        "\n",
        "**answer:** Early stopping, data augmentation, stronger regularization / dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyOU5dc18zkD"
      },
      "outputs": [],
      "source": [
        "# Now we evaluate the model on all the test set.\n",
        "loss, iou, miou = test(model, test_loader, criterion, metric)\n",
        "print(\">>>> [FINAL TEST on the test set: ] Avg. loss: \", loss ,\" | Mean IoU: \", miou)\n",
        "# Print per class IoU on last epoch or if best iou\n",
        "class_encoding = {c.name: c.id for c in train_set.classes if c.id < num_classes}\n",
        "for key, class_iou in zip(class_encoding.keys(), iou, strict=True):\n",
        "  print(f\"{key}: {class_iou:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gPJw158zkD"
      },
      "source": [
        "## B. Uncertainty evaluations with MCP\n",
        "Here you will just use as confidence score the Maximum class probability (MCP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L4fmecp8zkD"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\t# Forward propagation\n",
        "\toutputs = model(batch_img)\n",
        "\toutputs_proba = outputs.softmax(dim=1)\n",
        "\t# remove the batch dimension\n",
        "\toutputs_proba = outputs_proba.squeeze(0)\n",
        "\tconfidence, pred = outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afn94s1m8zkD"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTLLCpTQ8zkD"
      },
      "source": [
        "**Q12/ The last image is the related to the confidence score of the DNN. Can you explain why? What does the birght areas represent and what does the dark areas represent?**\n",
        "\n",
        "**answer:** The confidence map shows the maximum predicted class probability per pixel (MCP); bright areas correspond to pixels where the model is very confident in its prediction (high max softmax value), while dark areas indicate uncertain regions, typically near boundaries, small objects, or confusing part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2lBaYGD8zkD"
      },
      "source": [
        "### Now let's load the OOD test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EK6l_yi8zkE"
      },
      "outputs": [],
      "source": [
        "test_ood_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"ood\" , transforms=val_transform, download=True)\n",
        "test_ood_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oonIF23g8zkE"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "img, target = test_ood_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\t# Forward propagation\n",
        "\toutputs = model(batch_img)\n",
        "\toutputs_proba = outputs.softmax(dim=1)\n",
        "\t# remove the batch dimension\n",
        "\toutputs_proba = outputs_proba.squeeze(0)\n",
        "\tconfidence, pred = outputs_proba.max(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikr2WKe98zkE"
      },
      "outputs": [],
      "source": [
        "# Undo normalization on the image and convert to uint8.\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img_pil = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOhtqIgm8zkE"
      },
      "source": [
        "**According to the output is the model confident when it comes to labeling the bear and goat ? How about the bench ?**\n",
        "\n",
        "**answer:** The model seems to be as confident as other elements of same size for the bear and the goat (except around the boundary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fvvIDQd8zkE"
      },
      "source": [
        "\n",
        "**Q12 bis/ The last image is the related to the confidence score of the DNN. Can you explain why?**\n",
        "**Are you happy with this image?**\n",
        "\n",
        "**answer:** The image is relativly bright because MCP shows high softmax probabilities even when their is label errors.\n",
        "No, this is not good: it means the model is overconfident and not really calibrated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLGo9SKQ8zkE"
      },
      "source": [
        "## C. Uncertainty evaluations with Temperature Scaling\n",
        "**Q13/ please implement a temperature scaling using torch_uncertainty**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R2towX48zkF"
      },
      "source": [
        "Before Temprature scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_uncertainty.metrics import CalibrationError\n",
        "\n",
        "model.eval()\n",
        "ece_metric = CalibrationError(task=\"binary\", norm=\"l1\").cuda()\n",
        "\n",
        "all_confidences = []\n",
        "all_correct = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.cuda()\n",
        "        y = y.cuda().squeeze(1)\n",
        "\n",
        "        logits = model(x)\n",
        "        probs = logits.softmax(dim=1)\n",
        "        conf, pred = probs.max(1)\n",
        "\n",
        "        mask = (y != 255) & (y < 19)\n",
        "        correct = (pred == y) & mask\n",
        "\n",
        "        all_confidences.append(conf[mask])\n",
        "        all_correct.append(correct[mask])\n",
        "\n",
        "confidences = torch.cat(all_confidences)\n",
        "correct = torch.cat(all_correct)\n",
        "\n",
        "ece_before = ece_metric(confidences, correct.float())\n",
        "print(\"ECE before TS:\", ece_before.item())\n"
      ],
      "metadata": {
        "id": "5I7-7Jt28IAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "img, target = test_ood_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(batch_img)\n",
        "    outputs_proba = outputs.softmax(dim=1)\n",
        "    outputs_proba = outputs_proba.squeeze(0)\n",
        "    confidence, pred = outputs_proba.max(0)\n",
        "\n",
        "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
        "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
        "img = img * std[:, None, None] + mean[:, None, None]\n",
        "img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "tmp_target = target.masked_fill(target == 255, 21)\n",
        "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
        "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "img_pil = F.to_pil_image(img)\n",
        "img_segmented = F.to_pil_image(img_segmented)\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(img_segmented)\n",
        "ax3.imshow(pred_img)\n",
        "ax4.imshow(confidence_img)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vm72DBC5-czM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e8GJiWI8zkF"
      },
      "source": [
        "**Seeing the two graphs above comment on the MCP unceratinty result, is the model overconfident or calibrated ?**\n",
        "\n",
        "**answer:** The MCP uncertainty shows that the model tends to be overconfident before TS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKXrWIxo8zkF"
      },
      "source": [
        "After temperature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v_bTeuX8zkF"
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.post_processing import TemperatureScaler\n",
        "\n",
        "class SegmentationLogitLoader:\n",
        "    def __init__(self, dataloader, num_classes):\n",
        "        self.dataloader = dataloader\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        for img, labels in self.dataloader:\n",
        "            img = img.cuda()\n",
        "            labels = labels.squeeze(1).cuda()\n",
        "\n",
        "            labels = torch.where(labels == 255, 0, labels)\n",
        "\n",
        "            assert labels.max() < self.num_classes, \\\n",
        "                f\"Label max {labels.max()} >= num_classes {self.num_classes}\"\n",
        "\n",
        "            yield img, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "\n",
        "scaler = TemperatureScaler(model=model, device=\"cuda\")\n",
        "scaler.fit(SegmentationLogitLoader(val_loader, num_classes), progress=True)\n",
        "print(f\"Learned temperatures: {scaler.temperature[0].item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_KLW4uf8zkF"
      },
      "source": [
        "Now let's see the new confidence score image after scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIXNjK3c8zkG"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = scaler(batch_img)\n",
        "    outputs_proba = outputs.softmax(dim=1)\n",
        "    outputs_proba = outputs_proba.squeeze(0)\n",
        "    confidence, pred = outputs_proba.max(0)\n",
        "\n",
        "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
        "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
        "\n",
        "confidence_img = F.to_pil_image(confidence)\n",
        "pred_img = F.to_pil_image(pred_img)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12))\n",
        "ax1.imshow(img_pil)\n",
        "ax2.imshow(pred_img)\n",
        "ax3.imshow(confidence_img)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ZyTEsZ8zkG"
      },
      "source": [
        "**Did the model get more confident ? or is it more calibrated ? Commnet on the temperature scaling graphs and results**\n",
        "\n",
        "**answer:** The model becomes a bit less confident when classifying the bear or the goat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDWeuhIh8zkG"
      },
      "source": [
        "## D. Uncertainty evaluations with MC Dropout\n",
        "\n",
        "Let us implement **MC dropout**. This technique decribed in [this paper](https://arxiv.org/abs/1506.02142) allow us to have a better confindence score by using the dropout during test time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCcmyJ0V8zkG"
      },
      "source": [
        "**Q\\14 Please implement MC Dropout using torch_uncertainty**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZQO-FDL8zkG"
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.models.wrappers.mc_dropout import mc_dropout\n",
        "\n",
        "def predict_with_mc_dropout(mc_model, image, num_estimators=20):\n",
        "    \"\"\"\n",
        "    Perform MC Dropout inference\n",
        "    Returns: mean prediction, uncertainty (variance or entropy)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        logits = mc_model(image)  # [num_estimators, B, C, H, W]\n",
        "\n",
        "\n",
        "        if logits.dim() == 4:  # [T, C, H, W]\n",
        "            logits = logits.unsqueeze(1)  # [T, 1, C, H, W]\n",
        "\n",
        "        probs = torch.softmax(logits, dim=2)\n",
        "\n",
        "    mean_pred = probs.mean(dim=0)  # [B, C, H, W]\n",
        "    variance = probs.var(dim=0).sum(dim=1)  # [B, H, W]\n",
        "    entropy = -(mean_pred * torch.log(mean_pred + 1e-10)).sum(dim=1)  # [B, H, W]\n",
        "\n",
        "    return mean_pred, variance, entropy\n",
        "\n",
        "results = {}\n",
        "\n",
        "for T in [3, 20]:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MC Dropout with T={T} estimators\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    mc_model_T = mc_dropout(\n",
        "        core_model=model,\n",
        "        num_estimators=T,\n",
        "        last_layer=False,\n",
        "        on_batch=False,\n",
        "        task=\"segmentation\"\n",
        "    )\n",
        "    mc_model_T.eval()\n",
        "    mc_model_T.cuda()\n",
        "\n",
        "    img, label = next(iter(val_loader))\n",
        "    img = img[:1].cuda()\n",
        "    label = label[:1].cuda()\n",
        "\n",
        "    mean_pred, variance, entropy = predict_with_mc_dropout(mc_model_T, img, T)\n",
        "    confidence, pred_class = mean_pred.max(dim=1)  # [1, H, W]\n",
        "\n",
        "    # avg metrics\n",
        "    avg_entropy = entropy.mean().item()\n",
        "    avg_variance = variance.mean().item()\n",
        "    avg_confidence = confidence.mean().item()\n",
        "\n",
        "    results[T] = {\n",
        "        'confidence': avg_confidence,\n",
        "        'entropy': avg_entropy,\n",
        "        'variance': avg_variance\n",
        "    }\n",
        "\n",
        "    print(f\"Average confidence: {avg_confidence:.4f}\")\n",
        "    print(f\"Average entropy: {avg_entropy:.4f}\")\n",
        "    print(f\"Average variance: {avg_variance:.6f}\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    img_np = img[0].cpu().permute(1, 2, 0).numpy()\n",
        "    mean_norm = np.array([0.485, 0.456, 0.406])\n",
        "    std_norm = np.array([0.229, 0.224, 0.225])\n",
        "    img_np = std_norm * img_np + mean_norm\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title('Input Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    gt_img = label[0].cpu().squeeze().numpy()\n",
        "    axes[1].imshow(gt_img, cmap='tab20', vmin=0, vmax=20)\n",
        "    axes[1].set_title('Ground Truth')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    pred_img = pred_class[0].cpu().numpy()\n",
        "    print(f\"pred_img shape for imshow: {pred_img.shape}\")  # Devrait Ãªtre (H, W)\n",
        "    axes[2].imshow(pred_img, cmap='tab20', vmin=0, vmax=20)\n",
        "    axes[2].set_title(f'Prediction (T={T})')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    conf_img = confidence[0].cpu().numpy()\n",
        "    im3 = axes[3].imshow(conf_img, cmap='hot', vmin=0, vmax=1)\n",
        "    axes[3].set_title(f'Confidence\\n(avg={avg_confidence:.3f})')\n",
        "    axes[3].axis('off')\n",
        "    plt.colorbar(im3, ax=axes[3], fraction=0.046)\n",
        "\n",
        "    entropy_img = entropy[0].cpu().numpy()\n",
        "    im4 = axes[4].imshow(entropy_img, cmap='viridis')\n",
        "    axes[4].set_title(f'Uncertainty (Entropy)\\n(avg={avg_entropy:.3f})')\n",
        "    axes[4].axis('off')\n",
        "    plt.colorbar(im4, ax=axes[4], fraction=0.046)\n",
        "\n",
        "    variance_img = variance[0].cpu().numpy()\n",
        "    im5 = axes[5].imshow(variance_img, cmap='plasma')\n",
        "    axes[5].set_title(f'Variance\\n(avg={avg_variance:.4f})')\n",
        "    axes[5].axis('off')\n",
        "    plt.colorbar(im5, ax=axes[5], fraction=0.046)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'mc_dropout_T{T}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"COMPARISON: T=3 vs T=20\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"{'Metric':<20} {'T=3':<12} {'T=20':<12} {'Change'}\")\n",
        "print(f\"{'-'*60}\")\n",
        "\n",
        "for metric in ['confidence', 'entropy', 'variance']:\n",
        "    val_3 = results[3][metric]\n",
        "    val_20 = results[20][metric]\n",
        "    change = ((val_20 - val_3) / val_3) * 100\n",
        "    print(f\"{metric.capitalize():<20} {val_3:<12.4f} {val_20:<12.4f} {change:+.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqeweZaq8zkH"
      },
      "source": [
        "**Try the MC dropout code with a low number of estimators T like 3 and a high number 20, Explain the diffrence seen on the confidence image, is the model getting more confident or less ?**\n",
        "\n",
        "**answer:** With T=20, the model becomes slightly less confident compared to T=3, which is actually a good thing because more estimators give a more reliable measure of where the model is truly uncertain, especially at object boundaries and difficult areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf7ifKb8zkH"
      },
      "source": [
        "## E. Uncertainty evaluations with Deep Ensembles\n",
        "**Q\\15 Please implement [Deep Ensembles](https://papers.nips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf).**\n",
        "\n",
        "\n",
        "1.   You need to train 3 DNNs and save it. (Go back to the training cell above and train and save 3 diffrent models)\n",
        "2.   Use TorchUncertainty to get predictions\n",
        "\n",
        "You have two options either train several models using the code above or use TU to train the ensemble of models in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7r8KMm08zkH"
      },
      "outputs": [],
      "source": [
        "from torch_uncertainty.models import deep_ensembles\n",
        "\n",
        "saved_model_paths = ['model1.pth', 'model2.pth', 'model3.pth']\n",
        "ensemble_models = []\n",
        "\n",
        "for i, model_path in enumerate(saved_model_paths):\n",
        "    model_i = UNet(num_classes)\n",
        "    model_i.load_state_dict(torch.load(model_path))\n",
        "    model_i.eval()\n",
        "    model_i.cuda()\n",
        "    ensemble_models.append(model_i)\n",
        "    print(f\"Loaded model {i+1} from {model_path}\")\n",
        "\n",
        "ensemble = deep_ensembles(ensemble_models, task=\"segmentation\")\n",
        "ensemble.eval()\n",
        "ensemble.cuda()\n",
        "\n",
        "print(f\"\\nDeep Ensemble created with {len(ensemble_models)} models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWTayHkn8zkI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "img_np = img[0].cpu().permute(1, 2, 0).numpy()\n",
        "mean_norm = np.array([0.485, 0.456, 0.406])\n",
        "std_norm = np.array([0.229, 0.224, 0.225])\n",
        "img_np = std_norm * img_np + mean_norm\n",
        "img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "axes[0].imshow(img_np)\n",
        "axes[0].set_title('Input Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(label[0].cpu().squeeze(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[1].set_title('Ground Truth')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(pred_class[0].cpu(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[2].set_title(f'Ensemble Prediction ({len(ensemble_models)} models)')\n",
        "axes[2].axis('off')\n",
        "\n",
        "im3 = axes[3].imshow(confidence[0].cpu(), cmap='hot', vmin=0, vmax=1)\n",
        "axes[3].set_title(f'Confidence (avg={confidence.mean().item():.3f})')\n",
        "axes[3].axis('off')\n",
        "plt.colorbar(im3, ax=axes[3], fraction=0.046)\n",
        "\n",
        "im4 = axes[4].imshow(entropy[0].cpu(), cmap='viridis')\n",
        "axes[4].set_title(f'Uncertainty/Entropy (avg={entropy.mean().item():.3f})')\n",
        "axes[4].axis('off')\n",
        "plt.colorbar(im4, ax=axes[4], fraction=0.046)\n",
        "\n",
        "im5 = axes[5].imshow(variance[0].cpu(), cmap='plasma')\n",
        "axes[5].set_title(f'Variance (avg={variance.mean().item():.4f})')\n",
        "axes[5].axis('off')\n",
        "plt.colorbar(im5, ax=axes[5], fraction=0.046)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6VjlLJ08zkI"
      },
      "source": [
        "Test your ensemble obtained either using option 1 or 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvVP-Xdr8zkI"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img, label = next(iter(val_loader))\n",
        "    img = img.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "    ensemble_output = ensemble(img) # (num_models, batch_size, num_classes, H, W)\n",
        "\n",
        "    mean_logits = ensemble_output.mean(dim=0)  # avg over models\n",
        "    pred_class = mean_logits.argmax(dim=1)\n",
        "\n",
        "    probs = torch.softmax(mean_logits, dim=1)\n",
        "    confidence, _ = probs.max(dim=1)\n",
        "    entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)\n",
        "\n",
        "    # calculate variance across ensemble predictions\n",
        "    all_probs = torch.softmax(ensemble_output, dim=2)  # (num_models, batch, classes, H, W)\n",
        "    variance = all_probs.var(dim=0).mean(dim=1)\n",
        "\n",
        "print(f\"Prediction shape: {pred_class.shape}\")\n",
        "print(f\"Confidence shape: {confidence.shape}\")\n",
        "print(f\"Entropy shape: {entropy.shape}\")\n",
        "print(f\"Variance shape: {variance.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Tc7bav8zkJ"
      },
      "source": [
        "Save the ensemble model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ9y8Woz8zkJ"
      },
      "outputs": [],
      "source": [
        "final_model_path = \"ensemble.pth\"\n",
        "torch.save(ensemble.state_dict(), final_model_path)\n",
        "print(f\"Model saved to {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZf8B6xf8zkJ"
      },
      "source": [
        "## F. Uncertainty evaluations with Packed-Ensembles\n",
        "**Q\\15 Please read [Packed-Ensembles](https://arxiv.org/pdf/2210.09184). Then Implement a Packed-Ensembles Unet and train it and evaluate its Uncertainty**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74wEu5jG8zkJ"
      },
      "outputs": [],
      "source": [
        "class PackedUp(nn.Module):\n",
        "    def __init__(self, in_ch, skip_ch, out_ch, pack=2, bilinear=True):\n",
        "        super().__init__()\n",
        "        assert out_ch % pack == 0\n",
        "\n",
        "        self.pack = pack\n",
        "        mid_ch = out_ch // pack\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
        "            self.up_proj = nn.Conv2d(in_ch, mid_ch * pack, 1, bias=False)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch, mid_ch * pack, 2, stride=2)\n",
        "            self.up_proj = nn.Identity()\n",
        "\n",
        "        self.skip_proj = nn.Conv2d(skip_ch, mid_ch * pack, 1, bias=False)\n",
        "        self.merge = nn.Conv2d(mid_ch * 2 * pack, out_ch, 1, bias=False)\n",
        "        self.conv = DoubleConv(out_ch, out_ch)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = self.up_proj(x)\n",
        "        skip = self.skip_proj(skip)\n",
        "\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, self.pack, C // self.pack, H, W)\n",
        "        skip = skip.view(B, self.pack, C // self.pack, H, W)\n",
        "\n",
        "        x = torch.cat([x, skip], dim=2)\n",
        "        x = x.view(B, -1, H, W)\n",
        "        x = self.merge(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "class PackedUNet(nn.Module):\n",
        "    def __init__(self, classes, base_ch=32, pack=2):\n",
        "        super().__init__()\n",
        "\n",
        "        c1 = base_ch\n",
        "        c2 = c1 * 2\n",
        "        c3 = c2 * 2\n",
        "        c4 = c3 * 2\n",
        "        c5 = c4\n",
        "\n",
        "        self.inc = DoubleConv(3, c1)\n",
        "        self.down1 = Down(c1, c2)\n",
        "        self.down2 = Down(c2, c3)\n",
        "        self.down3 = Down(c3, c4)\n",
        "        self.down4 = Down(c4, c5)\n",
        "\n",
        "        self.up1 = PackedUp(c5, c4, c3, pack=pack)\n",
        "        self.up2 = PackedUp(c3, c3, c2, pack=pack)\n",
        "        self.up3 = PackedUp(c2, c2, c1, pack=pack)\n",
        "        self.up4 = PackedUp(c1, c1, c1, pack=pack)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.1)\n",
        "        self.outc = OutConv(c1, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.dropout(x)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.dropout(x)\n",
        "        return self.outc(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PackedUNet(classes=num_classes, pack=4).cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=255)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay_epochs, gamma=lr_decay)\n",
        "metric = MeanIntersectionOverUnion(num_classes=num_classes, ignore_index=255).cuda()"
      ],
      "metadata": {
        "id": "35AOo7WCEskD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "train_losses, val_losses = [], []\n",
        "train_mious, val_mious = [], []\n",
        "train_ious, val_ious = [], []\n",
        "\n",
        "best_val_miou = -1.0\n",
        "best_state = None\n",
        "\n",
        "# used 15 epoch rather than initial nb_epochs cuz too long on my google colab...\n",
        "nb_epochs = 20\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "    tr_loss, tr_iou_per_class, tr_miou = train(\n",
        "        model, train_loader, optimizer, criterion, metric, iteration_loss=False\n",
        "    )\n",
        "    va_loss, va_iou_per_class, va_miou = test(\n",
        "        model, val_loader, criterion, metric, iteration_loss=False\n",
        "    )\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    train_losses.append(tr_loss)\n",
        "    val_losses.append(va_loss)\n",
        "    train_mious.append(tr_miou.item() if hasattr(tr_miou, \"item\") else float(tr_miou))\n",
        "    val_mious.append(va_miou.item() if hasattr(va_miou, \"item\") else float(va_miou))\n",
        "    train_ious.append(tr_iou_per_class.detach().cpu())\n",
        "    val_ious.append(va_iou_per_class.detach().cpu())\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch:03d}/{nb_epochs:03d}] \"\n",
        "        f\"lr={scheduler.get_last_lr()[0]:.2e} | \"\n",
        "        f\"train: loss={tr_loss:.4f}, mIoU={train_mious[-1]:.4f} | \"\n",
        "        f\"val: loss={va_loss:.4f}, mIoU={val_mious[-1]:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_mious[-1] > best_val_miou:\n",
        "        best_val_miou = val_mious[-1]\n",
        "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "# restore best model (optional but usually expected)\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "print(f\"\\nBest val mIoU: {best_val_miou:.4f}\")\n",
        "\n",
        "# save model\n",
        "torch.save(model.state_dict(), \"model2.pth\")"
      ],
      "metadata": {
        "id": "DoyCHXe9AHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PackedUNet(num_classes, pack=4)\n",
        "model.load_state_dict(torch.load(\"model2.pth\"))"
      ],
      "metadata": {
        "id": "g7ZvT2OEU3ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict(model, image):\n",
        "    logits = model(image)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "    pred_class = probs.argmax(dim=1)\n",
        "    confidence = probs.max(dim=1).values\n",
        "    entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1)\n",
        "    variance = probs.var(dim=1)\n",
        "\n",
        "    return pred_class, confidence, entropy, variance\n"
      ],
      "metadata": {
        "id": "3nQqaiIVTL2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class, confidence, entropy, variance = predict(model, img)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "img_np = img[0].cpu().permute(1, 2, 0).numpy()\n",
        "mean_norm = np.array([0.485, 0.456, 0.406])\n",
        "std_norm = np.array([0.229, 0.224, 0.225])\n",
        "img_np = std_norm * img_np + mean_norm\n",
        "img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "axes[0].imshow(img_np)\n",
        "axes[0].set_title('Input Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(label[0].cpu().squeeze(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[1].set_title('Ground Truth')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(pred_class[0].cpu(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[2].set_title('Prediction')\n",
        "axes[2].axis('off')\n",
        "\n",
        "im3 = axes[3].imshow(confidence[0].cpu(), cmap='hot', vmin=0, vmax=1)\n",
        "axes[3].set_title(f'Confidence (avg={confidence.mean().item():.3f})')\n",
        "axes[3].axis('off')\n",
        "plt.colorbar(im3, ax=axes[3], fraction=0.046)\n",
        "\n",
        "im4 = axes[4].imshow(entropy[0].cpu(), cmap='viridis')\n",
        "axes[4].set_title(f'Uncertainty/Entropy (avg={entropy.mean().item():.3f})')\n",
        "axes[4].axis('off')\n",
        "plt.colorbar(im4, ax=axes[4], fraction=0.046)\n",
        "\n",
        "im5 = axes[5].imshow(variance[0].cpu(), cmap='plasma')\n",
        "axes[5].set_title(f'Variance (avg={variance.mean().item():.4f})')\n",
        "axes[5].axis('off')\n",
        "plt.colorbar(im5, ax=axes[5], fraction=0.046)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ownML6n3TOY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "img, target = test_ood_set[sample_idx]\n",
        "\n",
        "batch_img = img.unsqueeze(0).cuda()\n",
        "batch_target = target.unsqueeze(0).cuda()\n",
        "\n",
        "pred_class, confidence, entropy, variance = predict(model, batch_img)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "img_np = batch_img[0].cpu().permute(1, 2, 0).numpy()\n",
        "mean_norm = np.array([0.485, 0.456, 0.406])\n",
        "std_norm = np.array([0.229, 0.224, 0.225])\n",
        "img_np = std_norm * img_np + mean_norm\n",
        "img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "axes[0].imshow(img_np)\n",
        "axes[0].set_title('Input Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(batch_target[0].cpu().squeeze(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[1].set_title('Ground Truth')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(pred_class[0].cpu(), cmap='tab20', vmin=0, vmax=20)\n",
        "axes[2].set_title('Prediction')\n",
        "axes[2].axis('off')\n",
        "\n",
        "im3 = axes[3].imshow(confidence[0].cpu(), cmap='hot', vmin=0, vmax=1)\n",
        "axes[3].set_title(f'Confidence (avg={confidence.mean().item():.3f})')\n",
        "axes[3].axis('off')\n",
        "plt.colorbar(im3, ax=axes[3], fraction=0.046)\n",
        "\n",
        "im4 = axes[4].imshow(entropy[0].cpu(), cmap='viridis')\n",
        "axes[4].set_title(f'Uncertainty/Entropy (avg={entropy.mean().item():.3f})')\n",
        "axes[4].axis('off')\n",
        "plt.colorbar(im4, ax=axes[4], fraction=0.046)\n",
        "\n",
        "im5 = axes[5].imshow(variance[0].cpu(), cmap='plasma')\n",
        "axes[5].set_title(f'Variance (avg={variance.mean().item():.4f})')\n",
        "axes[5].axis('off')\n",
        "plt.colorbar(im5, ax=axes[5], fraction=0.046)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IZ6C5YLFdJjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please conclude your report**\n",
        "\n",
        "In this lab, we tested several techniques to improve segmentation and to handle OOD data. We compared deterministic methods like deep sets and temperature scaling with stochastic approaches such as MC Dropout and packed ensembles. The packed ensemble performed the best, giving accurate segmentations and the most reliable uncertainty estimates. MC Dropout also produced useful uncertainty maps, while temperature scaling mainly helped to calibrate the predictions. Overall, our experiments show that combining model design and uncertainty-aware methods can improve segmentation and better detect OOD regions."
      ],
      "metadata": {
        "id": "czg42bYrRcG7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}